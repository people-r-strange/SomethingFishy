---
title: "DC_1"
output: html_document
author: Lauren Low
---

```{r loading packages}
library(tidyverse)
library(dplyr)
library(readxl)
library(stringr)
library(tidytext)
library(readr)
library(data.table)
library(vroom)
library(readtext)
```

```{r loading files and joinging dataframes}

conference <- read_excel("/Users/llow/Desktop/DC1-data/City Hall Phone Log/Conference Room Phone Log.xls", sheet = "Sheet1")
voter <- read_excel("/Users/llow/Desktop/DC1-data/Voter Registry/Voter Registry.xls", sheet = "Sheet1")
conference
voter
```

```{r manually reading in text as a vector}
# it would take alot of time to manually coy and paste each article into r
text <- c("
Alderwood to probe voting machines Story by: Ellie Olmsen Date Published to Web: 11/16/2004 Republicans in Alderwood joined Democrats yesterday in criticizing the performance of the city's costly new high-tech voting system, saying that it may have disenfranchised voters in the Nov. 4 election. The Republican commission scolded the city board of elections for minimizing problems with the touch-screen machines that the city purchased this year for $1.5 million and asked Mayor Rex Luthor to investigate what went wrong before the machines are pressed into service again. Alderwood's touch-screen voting machines, which resemble laptop computers without keyboards, were supposed to simplify voting and tabulating results. But in a debut that mirrored many of the problems experienced last year in areas across the country, some voters found the machines confusing, and the reporting of vote tallies was delayed almost a day. Luthor responded that he would try to address the board's concerns. He said he has called for a public meeting of the three-member board of elections to go over the requests at 5 p.m. today. I pledge that I will answer every question as soon as I possibly can in the proper fashion, he said."
)
```

```{r finding most common word in text}
# put text in a df
text_df <- data_frame(long_string = text) 

# makes each word its own column
word_col <- text_df %>% 
   unnest_tokens(output = word, input = long_string) 
 
# removes stop words like "the", "and", "before", "after", "such", "as", etc.
no_stops  <- word_col  %>%
  anti_join(stop_words)

# finds most common word and counts
common <- no_stops %>%
  count(word, sort = TRUE)
common
```

```{r reading in all of the text files}
# lets see if we can read in all the files in to News Articles folder so we don't have to copy and paste a bunch
dir <- "/Users/llow/Desktop/DC1-data/News Articles"
article = readtext(paste0(dir, "/*.txt"))
article
```

```{r analyszing all articles for most common words}
# put text in a df
text_df <- data_frame(long_string = article) 

# makes each word its own column
word_col <- text_df %>% 
   unnest_tokens(output = word, input = long_string) 
 
# removes stop words like "the", "and", "before", "after", "such", "as", etc.
no_stops  <- word_col  %>%
  anti_join(stop_words)

# finds most common word and counts
common <- no_stops %>%
  count(word, sort = TRUE)
common
```

figured out how to do bits of text analysis here: https://steemit.com/programming/@dkmathstats/finding-the-most-frequent-words-in-text-with-r
https://stackoverflow.com/questions/23413331/how-to-remove-last-n-characters-from-every-element-in-the-r-vector
